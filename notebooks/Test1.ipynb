{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "db28bcf8-000d-446f-a4e5-4c0a218ed51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools as ft\n",
    "import math\n",
    "import warnings\n",
    "from typing import Literal, Optional, Tuple, Union, overload, Callable\n",
    "\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.lax as lax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "from equinox.nn import Dropout, Linear, State, StateIndex\n",
    "from jaxtyping import Array, Bool, Float, PRNGKeyArray\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "40d342ee-1782-4819-b9b0-5d5ae2c2ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(x: Array, n_rep: int) -> Array:\n",
    "    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n",
    "    slen, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return jnp.repeat(x, n_rep, axis=1).reshape(slen, n_kv_heads * n_rep, head_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "74acb614-0450-41ac-a7be-69abecc4898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_attention_weights(\n",
    "    query: Float[Array, \"q_seq qk_size\"],\n",
    "    key: Float[Array, \"kv_seq qk_size\"],\n",
    "    mask: Optional[Bool[Array, \"q_seq kv_seq\"]] = None,\n",
    ") -> Float[Array, \"q_seq kv_seq\"]:\n",
    "    query = query / math.sqrt(query.shape[-1])\n",
    "    logits = jnp.einsum(\"sd,Sd->sS\", query, key)\n",
    "    if mask is not None:\n",
    "        if mask.shape != logits.shape:\n",
    "            raise ValueError(\n",
    "                f\"mask must have shape (query_seq_length, \"\n",
    "                f\"kv_seq_length)=({query.shape[0]}, \"\n",
    "                f\"{key.shape[0]}). Got {mask.shape}.\"\n",
    "            )\n",
    "        logits = jnp.where(mask, logits, jnp.finfo(logits.dtype).min)\n",
    "\n",
    "    return jax.nn.softmax(logits, axis=-1)\n",
    "\n",
    "\n",
    "def dot_product_attention(\n",
    "    query: Float[Array, \"q_seq qk_size\"],\n",
    "    key_: Float[Array, \"kv_seq qk_size\"],\n",
    "    value: Float[Array, \"kv_seq v_size\"],\n",
    "    mask: Optional[Bool[Array, \"q_seq kv_seq\"]] = None,\n",
    "    dropout: Optional[Dropout] = None,\n",
    "    *,\n",
    "    key: Optional[PRNGKeyArray] = None,\n",
    "    inference: Optional[bool] = None,\n",
    ") -> Float[Array, \"q_seq v_size\"]:\n",
    "    weights = dot_product_attention_weights(query, key_, mask)\n",
    "    if dropout is not None:\n",
    "        weights = dropout(weights, key=key, inference=inference)\n",
    "    attn = jnp.einsum(\"sS,Sd->sd\", weights, value)\n",
    "    return attn\n",
    "\n",
    "\n",
    "def vmapped_attention(\n",
    "    query_heads: Float[Array, \"q_seq qk_size\"],\n",
    "    key_heads: Float[Array, \"kv_seq qk_size\"],\n",
    "    value_heads: Float[Array, \"kv_seq v_size\"],\n",
    "    dropout: Optional[Dropout] = None,\n",
    "    inference: Optional[bool] = None,\n",
    "    mask: Optional[Float[Array, \"q_seq kv_seq\"]] = None,\n",
    "    keys: Optional[PRNGKeyArray] = None,\n",
    "):\n",
    "    attn_fn = ft.partial(\n",
    "        dot_product_attention, dropout=dropout, inference=inference, key=keys, mask=mask\n",
    "    )\n",
    "    # Batch `keys` down its first axis as it is passed as a keyword argument.\n",
    "    dpa = jax.vmap(\n",
    "        lambda q, k, v: attn_fn(q, k, v),\n",
    "        in_axes=(1, None, None),\n",
    "        out_axes=1,\n",
    "    )(query_heads, key_heads, value_heads)\n",
    "    return dpa\n",
    "\n",
    "\n",
    "class MultiheadAttention(eqx.Module):\n",
    "\n",
    "    query_proj: Linear\n",
    "    key_proj: Linear\n",
    "    value_proj: Linear\n",
    "    output_proj: Linear\n",
    "    dropout: Dropout\n",
    "    autoregressive_index: Optional[StateIndex]\n",
    "\n",
    "    num_heads: int = eqx.field(static=True)\n",
    "    query_size: int = eqx.field(static=True)\n",
    "    key_size: int = eqx.field(static=True)\n",
    "    value_size: int = eqx.field(static=True)\n",
    "    output_size: int = eqx.field(static=True)\n",
    "\n",
    "    state_length: Optional[int] = eqx.field(static=True)\n",
    "    qk_size: int = eqx.field(static=True)\n",
    "    vo_size: int = eqx.field(static=True)\n",
    "    use_query_bias: bool = eqx.field(static=True)\n",
    "    use_key_bias: bool = eqx.field(static=True)\n",
    "    use_value_bias: bool = eqx.field(static=True)\n",
    "    use_output_bias: bool = eqx.field(static=True)\n",
    "\n",
    "    kv_multihead_dim: int = eqx.field(static=True)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int,\n",
    "        query_size: int,\n",
    "        *,\n",
    "        key_size: Optional[int] = None,\n",
    "        value_size: Optional[int] = None,\n",
    "        output_size: Optional[int] = None,\n",
    "        kv_multihead_dim: Optional[int] = None,\n",
    "        state_length: Optional[int] = None,\n",
    "        qk_size: Optional[int] = None,\n",
    "        vo_size: Optional[int] = None,\n",
    "        use_query_bias: bool = False,\n",
    "        use_key_bias: bool = False,\n",
    "        use_value_bias: bool = False,\n",
    "        use_output_bias: bool = False,\n",
    "        dropout_p: float = 0.0,\n",
    "        inference: bool = False,\n",
    "        key: PRNGKeyArray,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        qkey, kkey, vkey, okey = jrandom.split(key, 4)\n",
    "\n",
    "        if key_size is None:\n",
    "            key_size = query_size\n",
    "        if value_size is None:\n",
    "            value_size = query_size\n",
    "        if qk_size is None:\n",
    "            qk_size = query_size // num_heads\n",
    "        if vo_size is None:\n",
    "            vo_size = query_size // num_heads\n",
    "        if output_size is None:\n",
    "            output_size = query_size\n",
    "\n",
    "        def _make_autoregressive_cache(**_):\n",
    "            if state_length is None:\n",
    "                raise ValueError(\n",
    "                    \"Cannot use autoregressive decoding without specifying \"\n",
    "                    \"`MultiheadAttention(..., state_length=...)`.\"\n",
    "                )\n",
    "            if kv_multihead_dim:\n",
    "                key_shape = state_length, num_heads, qk_size\n",
    "                value_shape = state_length, num_heads, vo_size\n",
    "            else:\n",
    "                key_shape = state_length, qk_size\n",
    "                value_shape = state_length, vo_size\n",
    "            if jax.config.jax_enable_x64:  # pyright: ignore\n",
    "                _int = jnp.int64\n",
    "            else:\n",
    "                _int = jnp.int32\n",
    "            return jnp.empty(key_shape), jnp.empty(value_shape), jnp.zeros((), _int)\n",
    "\n",
    "        query_proj_out_size = qk_size\n",
    "        key_proj_out_size = qk_size\n",
    "        value_proj_out_size = vo_size\n",
    "\n",
    "        kv_multihead_dim = num_heads if kv_multihead_dim is None else kv_multihead_dim\n",
    "\n",
    "        query_proj_out_size = query_proj_out_size * num_heads\n",
    "        key_proj_out_size = key_proj_out_size * kv_multihead_dim\n",
    "        value_proj_out_size = value_proj_out_size * kv_multihead_dim\n",
    "\n",
    "        self.query_proj = Linear(\n",
    "            query_size, query_proj_out_size, use_bias=use_query_bias, key=qkey\n",
    "        )\n",
    "        self.key_proj = Linear(\n",
    "            key_size, key_proj_out_size, use_bias=use_key_bias, key=kkey\n",
    "        )\n",
    "        self.value_proj = Linear(\n",
    "            value_size, value_proj_out_size, use_bias=use_value_bias, key=vkey\n",
    "        )\n",
    "\n",
    "        self.output_proj = Linear(\n",
    "            vo_size * num_heads, output_size, use_bias=use_output_bias, key=okey\n",
    "        )\n",
    "        self.dropout = Dropout(dropout_p, inference=inference)\n",
    "        self.autoregressive_index = StateIndex(_make_autoregressive_cache()) if state_length is not None else None\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.query_size = query_size\n",
    "\n",
    "        self.key_size = key_size\n",
    "        self.value_size = value_size\n",
    "        self.output_size = output_size\n",
    "        self.qk_size = qk_size\n",
    "        self.vo_size = vo_size\n",
    "        self.use_query_bias = use_query_bias\n",
    "        self.use_key_bias = use_key_bias\n",
    "        self.use_value_bias = use_value_bias\n",
    "        self.use_output_bias = use_output_bias\n",
    "        self.state_length = state_length\n",
    "        self.kv_multihead_dim = kv_multihead_dim\n",
    "        \n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        query: Float[Array, \"q_seq q_size\"],\n",
    "        key_: Float[Array, \"kv_seq k_size\"],\n",
    "        value: Float[Array, \"kv_seq v_size\"],\n",
    "        mask: Union[\n",
    "            None,\n",
    "            Bool[Array, \"q_seq kv_seq\"],\n",
    "            Bool[Array, \"num_heads q_seq kv_seq\"],\n",
    "            Literal[\"causal\"],\n",
    "        ] = None,\n",
    "        state: Optional[State] = None,\n",
    "        *,\n",
    "        key: Optional[PRNGKeyArray] = None,\n",
    "        inference: Optional[bool] = None,\n",
    "        deterministic: Optional[bool] = None,\n",
    "        process_heads: Optional[\n",
    "            Callable[\n",
    "                [\n",
    "                    Float[Array, \"query_size num_heads qk_size\"],\n",
    "                    Float[Array, \"key_size num_heads qk_size\"],\n",
    "                    Float[Array, \"value_size num_heads vo_size\"],\n",
    "                ],\n",
    "                Tuple[\n",
    "                    Float[Array, \"query_size num_heads qk_size\"],\n",
    "                    Float[Array, \"key_size num_heads qk_size\"],\n",
    "                    Float[Array, \"value_size num_heads vo_size\"],\n",
    "                ],\n",
    "            ]\n",
    "        ] = None,\n",
    "    ) -> Union[\n",
    "        Float[Array, \"q_seq o_size\"], Tuple[Float[Array, \"q_seq o_size\"], State]\n",
    "    ]:\n",
    "        if deterministic is not None:\n",
    "            inference = deterministic\n",
    "            warnings.warn(\n",
    "                \"MultiheadAttention()(deterministic=...) is deprecated \"\n",
    "                \"in favour of MultiheadAttention()(inference=...)\"\n",
    "            )\n",
    "\n",
    "        query_seq_length, _ = query.shape\n",
    "        kv_seq_length, _ = key_.shape\n",
    "        kv_seq_length2, _ = value.shape\n",
    "        if kv_seq_length != kv_seq_length2:\n",
    "            # query length can be different\n",
    "            raise ValueError(\"key and value must both be sequences of equal length.\")\n",
    "        del kv_seq_length2\n",
    "\n",
    "        # query_heads = self._project(self.query_proj, self.query_multihead, query)\n",
    "        # key_heads = self._project(self.key_proj, self.key_multihead, key_)\n",
    "        # value_heads = self._project(self.value_proj, self.value_multihead, value)\n",
    "\n",
    "        query_heads = self._new_project(\n",
    "            self.query_proj, self.num_heads, query\n",
    "        )\n",
    "        key_heads = self._new_project(self.key_proj, self.kv_multihead_dim, key_)\n",
    "        value_heads = self._new_project(self.value_proj, self.kv_multihead_dim, value)\n",
    "\n",
    "        if process_heads is not None:\n",
    "            q_shape, k_shape, v_shape = (\n",
    "                query_heads.shape,\n",
    "                key_heads.shape,\n",
    "                value_heads.shape,\n",
    "            )\n",
    "            query_heads, key_heads, value_heads = process_heads(\n",
    "                query_heads, key_heads, value_heads\n",
    "            )\n",
    "\n",
    "            if (\n",
    "                query_heads.shape != q_shape\n",
    "                or key_heads.shape != k_shape\n",
    "                or value_heads.shape != v_shape\n",
    "            ):\n",
    "                raise ValueError(\n",
    "                    \"process_heads must not change the shape of the heads.\"\n",
    "                )\n",
    "\n",
    "        if state is None:\n",
    "            causal_mask_offset = 0\n",
    "        else:\n",
    "            key_state, value_state, index = state.get(self.autoregressive_index)\n",
    "            key_state = lax.dynamic_update_slice_in_dim(\n",
    "                key_state, key_heads, index, axis=0\n",
    "            )\n",
    "            value_state = lax.dynamic_update_slice_in_dim(\n",
    "                value_state, value_heads, index, axis=0\n",
    "            )\n",
    "            causal_mask_offset = index\n",
    "            index = index + kv_seq_length\n",
    "            state = state.set(\n",
    "                self.autoregressive_index, (key_state, value_state, index)\n",
    "            )\n",
    "            key_heads = key_state\n",
    "            value_heads = value_state\n",
    "            kv_seq_length = self.state_length\n",
    "\n",
    "        if mask == \"causal\":\n",
    "            query_indices = jnp.arange(query_seq_length)[:, None]\n",
    "            kv_indices = jnp.arange(kv_seq_length)[None, :]\n",
    "            mask = kv_indices <= query_indices + causal_mask_offset\n",
    "        if state is not None:\n",
    "            # Also mask out the latter parts of the state we haven't written into yet.\n",
    "            unwritten_mask = jnp.arange(self.state_length) < index  # pyright: ignore\n",
    "            if mask is None:\n",
    "                mask = jnp.broadcast_to(\n",
    "                    unwritten_mask, (query_seq_length, self.state_length)\n",
    "                )\n",
    "            else:\n",
    "                mask = mask & unwritten_mask\n",
    "\n",
    "        keys = None if key is None else jax.random.split(key, self.num_heads)\n",
    "        if self.kv_multihead_dim == self.num_heads:\n",
    "            # Normal multi-head attention\n",
    "            attn_fn = ft.partial(\n",
    "                dot_product_attention, dropout=self.dropout, inference=inference\n",
    "            )\n",
    "            print(query_heads.shape, key_heads.shape, value_heads.shape)\n",
    "            in_axes = (1, 1, 1, 0 if mask is not None and mask.ndim == 3 else None)\n",
    "            attn = jax.vmap(\n",
    "                attn_fn, in_axes=in_axes, out_axes=1, axis_size=self.num_heads\n",
    "            )(query_heads, key_heads, value_heads, mask, key=keys)\n",
    "        else:\n",
    "            pt_vmapped_attention = ft.partial(\n",
    "                vmapped_attention,\n",
    "                dropout=self.dropout,\n",
    "                inference=inference,\n",
    "                mask=mask,\n",
    "                keys=keys,\n",
    "            )\n",
    "            attn = jax.vmap(pt_vmapped_attention, in_axes=(None, 1, 1), out_axes=1)(\n",
    "                query_heads, key_heads, value_heads\n",
    "            )\n",
    "\n",
    "            attn = jnp.sum(attn, axis=1)\n",
    "            # Taking the mean over the d dimension\n",
    "            attn = attn / self.kv_multihead_dim\n",
    "\n",
    "        attn = attn.reshape(query_seq_length, self.num_heads * self.vo_size)\n",
    "        out = jax.vmap(self.output_proj)(attn)\n",
    "\n",
    "        if state is None:\n",
    "            return out\n",
    "        else:\n",
    "            return out, state\n",
    "\n",
    "    def _project(self, proj, multihead, x):\n",
    "        seq_length, _ = x.shape\n",
    "        projection = jax.vmap(proj)(x)\n",
    "        if multihead:\n",
    "            _, projection_size = projection.shape\n",
    "            size_per_head = projection_size // self.num_heads\n",
    "            projection = projection.reshape(seq_length, self.num_heads, size_per_head)\n",
    "        return projection\n",
    "\n",
    "    def _new_project(self, proj: eqx.Module, multihead: int | None, x: Array) -> Array:\n",
    "        seq_length, _ = x.shape\n",
    "        projection = jax.vmap(proj)(x)\n",
    "\n",
    "        if multihead is not None:\n",
    "            _, projection_size = projection.shape\n",
    "            size_per_head = projection_size // multihead\n",
    "            projection = projection.reshape(seq_length, multihead, size_per_head)\n",
    "\n",
    "        return projection\n",
    "\n",
    "\n",
    "def self_attention(\n",
    "    num_heads: int,\n",
    "    size: int,\n",
    "    *,\n",
    "    multiquery: bool = False,\n",
    "    state_length: Optional[int] = None,\n",
    "    key: PRNGKeyArray,\n",
    "):\n",
    "    return MultiheadAttention(\n",
    "        num_heads=num_heads,\n",
    "        query_size=size,\n",
    "        state_length=state_length,\n",
    "        key_multihead=not multiquery,\n",
    "        value_multihead=not multiquery,\n",
    "        key=key,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a6dd2fa1-f3bc-4083-b5db-572e93d77509",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 6\n",
    "query_size = 12\n",
    "kv_multihead_dim = 6\n",
    "key = jax.random.PRNGKey(33)\n",
    "\n",
    "mha = MultiheadAttention(num_heads=num_heads, query_size=query_size, kv_multihead_dim=kv_multihead_dim, key=key)\n",
    "seq = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "08762569-3e9f-447f-a925-4caf9a175ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = jax.random.uniform(key=jax.random.PRNGKey(20), shape=(seq, query_size))\n",
    "key_ = jax.random.uniform(key=jax.random.PRNGKey(21), shape=(seq, query_size))\n",
    "value = jax.random.uniform(key=jax.random.PRNGKey(22), shape=(seq, query_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e292d944-7a44-4141-b255-039ad01e63fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 6, 2) (8, 6, 2) (8, 6, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8, 12)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = mha(query, key_, value)\n",
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dda47370-b36a-4f86-be86-a8e720045b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 12, 2)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2fc3edc5-c347-4bfb-945b-13dc1a42cd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 6\n",
    "query_size = 12\n",
    "kv_multihead_dim = 4\n",
    "key = jax.random.PRNGKey(33)\n",
    "\n",
    "mha = MultiheadAttention(num_heads=num_heads, query_size=query_size, kv_multihead_dim=kv_multihead_dim, key=key)\n",
    "seq = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1eb42126-ef6a-42ad-8615-a6e6035ad20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = jax.random.uniform(key=jax.random.PRNGKey(20), shape=(seq, query_size))\n",
    "key_ = jax.random.uniform(key=jax.random.PRNGKey(21), shape=(seq, query_size))\n",
    "value = jax.random.uniform(key=jax.random.PRNGKey(22), shape=(seq, query_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "853f489d-1811-4003-b183-001d96fa9f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 12)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = mha(query, key_, value)\n",
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "43a6d86f-92cb-4bb0-b8ed-970373be006d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8, 2)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = jnp.ones(shape=(seq, num_heads, 2))\n",
    "k = jnp.ones(shape=(seq, kv_multihead_dim, 2))\n",
    "v = jnp.ones(shape=(seq, kv_multihead_dim, 2))\n",
    "\n",
    "repeat_kv(k, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e8bb54-74bd-4fb4-8abc-0b54c82f2607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
